# AGENT IA - Sp√©cification Technique

> Surcouche IA pour le syst√®me de paris hippiques horse3

---

## üìã Invariants Projet (Ce qui ne doit JAMAIS changer)

### Invariant 1: R√©trocompatibilit√© API
> Les endpoints actuels restent identiques et continuent de renvoyer les m√™mes r√©sultats.

- ‚úÖ `/picks/today` ‚Üí Inchang√©
- ‚úÖ `/portfolio/today` ‚Üí Inchang√©
- ‚úÖ `/backtest/run` ‚Üí Inchang√©
- ‚úÖ Tous les endpoints existants dans `web/backend/main.py`

**Test de non-r√©gression:** Avant/apr√®s d√©ploiement, les r√©ponses API doivent √™tre identiques (hash JSON).

---

### Invariant 2: Pipeline ML = Source de V√©rit√©
> Le pipeline "scraper ‚Üí features ‚Üí mod√®le ‚Üí betting_policy ‚Üí API" reste la source de v√©rit√©.

```
scraper_pmu_simple.py ‚Üí prepare_ml_features.py ‚Üí XGBoost ‚Üí calibration_pipeline.py ‚Üí betting_policy.py
```

L'agent IA **consomme** les outputs de ce pipeline, il ne les **remplace pas**.

---

### Invariant 3: IA = Critique, pas Calcul
> La surcouche IA ne modifie pas les probabilit√©s ni les cotes.

| Interdit ‚ùå | Autoris√© ‚úÖ |
|------------|------------|
| Recalculer `p_model_win` | Critiquer si `p_model_win` semble incoh√©rent |
| Modifier `value_win` | Signaler si `value_win < 0` mais bet gard√© |
| Changer `cote_finale` | V√©rifier que la cote en DB = cote PMU r√©elle |
| Inventer des donn√©es | Proposer retrait/ajout avec justification explicite |

---

### Invariant 4: Tra√ßabilit√© Totale
> Toute d√©cision finale doit √™tre tra√ßable √©tape par √©tape dans l'admin.

Chaque ex√©cution de l'agent g√©n√®re:
- `run_id` unique
- Logs de chaque √©tape (A, B, C, D)
- Inputs/outputs JSON stock√©s
- Timestamps et dur√©es
- Evidence pour chaque claim

---

### Invariant 5: Anti-Hallucination
> L'IA doit √©viter hallucinations et surconfiance.

R√®gles strictes:
1. **Pas de fait sans source** ‚Üí Tout claim = preuve attach√©e (DB, API, Web)
2. **Incertitude explicite** ‚Üí "Non v√©rifi√©" si pas de preuve
3. **Outputs valid√©s** ‚Üí JSON valid√© par Pydantic, sinon rejet + retry
4. **Pas de martingale** ‚Üí Le syst√®me Kelly reste la r√®gle de mise

---

## üîÑ Pipeline Agent IA - 4 √âtapes

### √âtape A: G√©n√©ration Rapport Algo (Sans LLM)

**Input:** Date, profil utilisateur, bankroll

**Process:**
1. Appeler le pipeline existant (`betting_policy.select_portfolio_from_picks`)
2. Pour chaque pick: extraire d√©cision + raisons + r√®gles appliqu√©es
3. Structurer en JSON selon le sch√©ma d√©fini

**Output:** `AlgoReportJSON` (voir sch√©ma ci-dessous)

**Definition of Done:**
- [ ] JSON valide selon sch√©ma Pydantic
- [ ] Tous les champs obligatoires remplis
- [ ] Chaque decision a `status`, `why[]`, `failed_rules[]`
- [ ] Stock√© en DB table `agent_runs`
- [ ] Aucun appel LLM

---

### √âtape B: Analyse IA (LLM + Outils Internes)

**Input:** `AlgoReportJSON` de l'√©tape A

**Process:**
1. LLM re√ßoit le rapport JSON complet
2. LLM peut appeler des outils d√©finis:
   - `get_runner_history(runner_id, n=10)` ‚Üí Historique cheval
   - `get_race_context(race_id)` ‚Üí Conditions course
   - `recompute_constraints(report)` ‚Üí V√©rifier r√®gles
   - `explain_features(runner_id)` ‚Üí Feature importance
3. LLM analyse coh√©rence, anomalies, propositions

**Output:** `AnalysisReportJSON`
```json
{
  "anomalies": [{"pick_id": "...", "issue": "...", "severity": "HIGH|MEDIUM|LOW"}],
  "modifications": [{"pick_id": "...", "action": "REMOVE|REDUCE_STAKE|ADD", "reason": "..."}],
  "questions": ["..."]
}
```

**Definition of Done:**
- [ ] LLM a uniquement acc√®s aux outils whitelist
- [ ] Output valid√© par Pydantic
- [ ] Chaque anomalie a severity + justification
- [ ] Chaque modification a raison + evidence
- [ ] Stock√© en `agent_steps` (run_id, step="B")
- [ ] Dur√©e et tokens logg√©s

---

### √âtape C: V√©rification (LLM + Preuves)

**Input:** `AnalysisReportJSON` de l'√©tape B

**Process:**
1. Pour chaque claim de l'√©tape B ‚Üí chercher preuve
2. Sources autoris√©es:
   - **Interne:** DB horse3 (historique, courses, partants)
   - **Externe (optionnel):** API PMU, France Galop, LeTrot (lecture seule)
3. Marquer chaque claim comme `VERIFIED` ou `UNVERIFIED`

**Output:** `VerificationReportJSON`
```json
{
  "verified_claims": [{"claim": "...", "source": "DB|API|WEB", "evidence": {...}}],
  "unverified_claims": [{"claim": "...", "potential_impact": "HIGH|MEDIUM|LOW"}],
  "verification_rate": 0.85
}
```

**Definition of Done:**
- [ ] Chaque claim a un statut v√©rifi√©/non v√©rifi√©
- [ ] Chaque v√©rification a une source tra√ßable
- [ ] Pas de claim sans preuve marqu√© comme "v√©rifi√©"
- [ ] Evidence stock√©e dans `agent_evidence`
- [ ] `verification_rate` calcul√©

---

### √âtape D: Auto-Critique + Proposition Finale (LLM)

**Input:** Rapports des √©tapes A, B, C

**Process:**
1. LLM analyse sa propre analyse (m√©ta-r√©flexion)
2. Identifie forces / faiblesses / risques
3. Produit recommandation finale avec score de confiance
4. Compare avec picks algo original (diff)

**Output:** `FinalReportJSON`
```json
{
  "self_critique": {
    "strengths": ["..."],
    "weaknesses": ["..."],
    "remaining_questions": ["..."],
    "risk_notes": ["..."]
  },
  "final_bets": [
    {
      "race_id": "R3C5",
      "runner_id": "...",
      "bet_type": "PLACE",
      "stake_eur": 10,
      "expected_value": 0.15,
      "justification": "...",
      "verified_elements": ["..."]
    }
  ],
  "confidence_score": 72,
  "diff_vs_algo": {
    "kept": [...],
    "removed": [...],
    "modified": [...]
  }
}
```

**Definition of Done:**
- [ ] Self-critique pr√©sente avec les 4 cat√©gories
- [ ] Chaque bet final a justification compl√®te
- [ ] `confidence_score` calcul√© (pas "ressenti" LLM)
- [ ] Diff explicite vs picks algo original
- [ ] Stock√© en `agent_steps` + `agent_diffs`

---

## üìä Score de Confiance (Calcul√©, pas LLM)

Le score est d√©terministe, bas√© sur:

| Facteur | Poids | Mesure |
|---------|-------|--------|
| Qualit√© mod√®le | 20% | drift_status: OK=100, WARN=50, ALERT=0 |
| Marge value | 25% | (value - cutoff) / cutoff √ó 100, cap 100 |
| Consensus mod√®le/march√© | 20% | 1 - \|p_model - p_implied\| √ó 5 |
| Risque | 20% | 100 - (odds/max_odds √ó 50 + field_size/20 √ó 50) |
| Taux v√©rification | 15% | verification_rate √ó 100 |

**Formule finale:**
```python
confidence = (drift * 0.20 + value_margin * 0.25 + consensus * 0.20
              + risk_score * 0.20 + verif_rate * 0.15)
```

---

## üóÑÔ∏è Sch√©ma Base de Donn√©es

### Table `agent_runs`
```sql
CREATE TABLE agent_runs (
    run_id UUID PRIMARY KEY,
    date_run DATE NOT NULL,
    user_id INTEGER REFERENCES users(id),
    profile VARCHAR(20),  -- PRUDENT|STANDARD|AGRESSIF
    bankroll DECIMAL(10,2),
    status VARCHAR(20),   -- PENDING|RUNNING|SUCCESS|FAILED
    started_at TIMESTAMP,
    finished_at TIMESTAMP,
    algo_report JSONB,    -- √âtape A
    final_report JSONB,   -- √âtape D
    confidence_score INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Table `agent_steps`
```sql
CREATE TABLE agent_steps (
    step_id UUID PRIMARY KEY,
    run_id UUID REFERENCES agent_runs(run_id),
    step_name VARCHAR(1),  -- A|B|C|D
    input_json JSONB,
    output_json JSONB,
    llm_model VARCHAR(50),
    tokens_in INTEGER,
    tokens_out INTEGER,
    cost_usd DECIMAL(10,6),
    duration_ms INTEGER,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Table `agent_evidence`
```sql
CREATE TABLE agent_evidence (
    evidence_id UUID PRIMARY KEY,
    run_id UUID REFERENCES agent_runs(run_id),
    claim_id VARCHAR(100),
    claim_text TEXT,
    source_type VARCHAR(10),  -- DB|API|WEB
    source_url TEXT,
    payload JSONB,
    verified BOOLEAN,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Table `agent_diffs`
```sql
CREATE TABLE agent_diffs (
    diff_id UUID PRIMARY KEY,
    run_id UUID REFERENCES agent_runs(run_id),
    pick_id VARCHAR(100),
    action VARCHAR(20),  -- KEPT|REMOVED|MODIFIED|ADDED
    algo_decision JSONB,
    agent_decision JSONB,
    reason TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## üîå Endpoints API (Nouveaux)

| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/agent/run` | Lance pipeline complet |
| GET | `/agent/runs` | Liste tous les runs |
| GET | `/agent/runs/{run_id}` | D√©tails d'un run |
| GET | `/agent/runs/{run_id}/steps` | Toutes les √©tapes |
| GET | `/agent/runs/{run_id}/diff` | Diff algo vs agent |
| GET | `/agent/runs/{run_id}/evidence` | Preuves collect√©es |
| POST | `/agent/runs/{run_id}/replay` | Rejouer un run |

---

## üñ•Ô∏è Interface Admin

### Vue 1: Liste des Runs
- Tableau: date, status, #bets algo, #bets final, confiance, dur√©e
- Filtres: date, status, profil
- Actions: voir d√©tails, rejouer

### Vue 2: D√©tail Run (Timeline)
- Accord√©on par √©tape (A ‚Üí B ‚Üí C ‚Üí D)
- Pour chaque √©tape: input, output, dur√©e, tokens
- Onglet "Evidence": claims v√©rifi√©s + sources
- Onglet "Logs": trace compl√®te

### Vue 3: Diff Picks
- Colonne gauche: picks algo
- Colonne droite: picks agent
- Highlight: ajout√©s (vert), retir√©s (rouge), modifi√©s (orange)
- Pour chaque diff: raison + evidence

---

## ‚úÖ Checklist MVP

### Phase 1: Infrastructure
- [ ] Cr√©er tables DB (`agent_runs`, `agent_steps`, `agent_evidence`, `agent_diffs`)
- [ ] D√©finir mod√®les Pydantic pour tous les JSON
- [ ] Cr√©er endpoints API basiques

### Phase 2: √âtape A (Rapport Algo)
- [ ] Modifier `betting_policy.py` pour exporter d√©cisions structur√©es
- [ ] G√©n√©rer JSON complet avec `why[]` et `failed_rules[]`
- [ ] Stocker en DB

### Phase 3: √âtape B + D (Analyse + Final)
- [ ] Int√©grer LLM (OpenAI/Gemini)
- [ ] D√©finir tools/functions disponibles
- [ ] Impl√©menter analyse + auto-critique
- [ ] Calculer score de confiance

### Phase 4: Admin UI
- [ ] Vue liste runs
- [ ] Vue d√©tail timeline
- [ ] Vue diff picks

### Phase 5 (Optionnel): √âtape C (V√©rification externe)
- [ ] Connecter sources externes
- [ ] Impl√©menter collecte evidence
- [ ] Marquer claims v√©rifi√©s/non v√©rifi√©s

---

## üìù Notes d'Impl√©mentation

### Choix LLM recommand√© (qualit√©/prix)
- **GPT-4o-mini** pour analyse (bon rapport qualit√©/prix)
- **GPT-4o** ou **Claude 3.5 Sonnet** pour auto-critique (meilleur raisonnement)
- Alternative √©conomique: **Gemini 1.5 Flash**

### Estimation co√ªts (ordre de grandeur)
- ~5-10 courses/jour √ó 4 √©tapes √ó ~2-5K tokens = ~40-200K tokens/jour
- GPT-4o-mini: ~$0.15-0.60 / mille tokens ‚Üí ~$6-30/mois
- GPT-4o: ~$2.50-10 / mille tokens ‚Üí ~$100-500/mois

### Latence estim√©e
- √âtape A: <1s (pas de LLM)
- √âtape B: 5-15s (LLM + tools)
- √âtape C: 2-10s (selon sources externes)
- √âtape D: 5-10s (LLM)
- **Total: 15-40s par run**
