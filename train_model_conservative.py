#!/usr/bin/env python3
"""
ENTRA√éNEMENT MOD√àLE CONSERVATEUR "SAFE"
========================================

Entra√Æne un mod√®le XGBoost avec des hyperparam√®tres conservateurs
pour une strat√©gie de betting plus d√©fensive.

Diff√©rences vs mod√®le champion:
- Profondeur r√©duite (5 vs 8) ‚Üí moins de surajustement
- Learning rate faible (0.05 vs 0.1) ‚Üí convergence plus stable
- R√©gularisation forte (L1=0.5, L2=2.0) ‚Üí g√©n√©ralisation
- min_child_weight √©lev√© (10 vs 3) ‚Üí √©vite patterns rares

Usage:
    python train_model_conservative.py [--dry-run]
"""

import argparse
import json
import logging
import time
from datetime import datetime
from pathlib import Path

import joblib
import pandas as pd
import xgboost as xgb
from sklearn.metrics import (
    brier_score_loss,
    f1_score,
    log_loss,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.preprocessing import RobustScaler

# Configuration des logs
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# HYPERPARAM√àTRES CONSERVATEURS
# ============================================================================

CONSERVATIVE_PARAMS = {
    "objective": "binary:logistic",
    "eval_metric": ["auc", "logloss"],
    # Structure de l'arbre - CONSERVATEUR
    "max_depth": 5,  # R√©duit (vs 8) ‚Üí moins complexe
    "min_child_weight": 10,  # Augment√© (vs 3) ‚Üí patterns stables
    "gamma": 0.3,  # Co√ªt minimum pour split ‚Üí arbres simples
    # Apprentissage - LENT ET STABLE
    "learning_rate": 0.05,  # R√©duit (vs 0.1) ‚Üí convergence douce
    "n_estimators": 2000,  # Plus d'it√©rations (early stop)
    # R√©gularisation - FORTE
    "reg_alpha": 0.5,  # L1 (vs 0.1) ‚Üí s√©lection features
    "reg_lambda": 2.0,  # L2 (vs 1.0) ‚Üí poids plus petits
    # Subsampling - CONSERVATEUR
    "subsample": 0.7,  # R√©duit (vs 0.8)
    "colsample_bytree": 0.6,  # R√©duit (vs 0.8)
    "colsample_bylevel": 0.8,
    # Gestion d√©s√©quilibre classes
    "scale_pos_weight": 0.8,  # Sous-pond√®re victoires rares
    # Autres
    "random_state": 42,
    "n_jobs": -1,
    "verbosity": 0,
    "tree_method": "hist",  # Plus rapide
}

# Crit√®res de filtrage conservateurs
FILTER_CRITERIA = {
    "min_partants": 8,  # Exclure courses < 8 partants
    "max_partants": 18,  # Exclure tr√®s gros pelotons
    "min_horse_history": 5,  # Min 5 courses historiques
    "exclude_disciplines": [],  # Toutes disciplines OK
    "prefer_disciplines": ["PLAT", "ATTELE"],  # Plus pr√©visibles
}


# ============================================================================
# CHARGEMENT DES DONN√âES
# ============================================================================


def load_data(data_dir: Path = Path("data")):
    """Charge les donn√©es SAFE (sans data leakage)"""

    logger.info("üìÇ CHARGEMENT DES DATASETS SAFE")

    train_path = data_dir / "train_SAFE.csv"
    val_path = data_dir / "val_SAFE.csv"
    test_path = data_dir / "test_SAFE.csv"

    # V√©rifier existence
    for path in [train_path, val_path, test_path]:
        if not path.exists():
            raise FileNotFoundError(f"‚ùå Fichier manquant: {path}")

    train_df = pd.read_csv(train_path)
    val_df = pd.read_csv(val_path)
    test_df = pd.read_csv(test_path)

    logger.info(f"‚úÖ Train: {train_df.shape[0]:,} √ó {train_df.shape[1]}")
    logger.info(f"‚úÖ Val: {val_df.shape[0]:,} √ó {val_df.shape[1]}")
    logger.info(f"‚úÖ Test: {test_df.shape[0]:,} √ó {test_df.shape[1]}")

    return train_df, val_df, test_df


def apply_conservative_filters(df: pd.DataFrame) -> pd.DataFrame:
    """Applique les filtres conservateurs aux donn√©es"""

    initial_count = len(df)

    # Filtrer par nombre de partants si colonne existe
    if "nb_partants" in df.columns:
        df = df[
            (df["nb_partants"] >= FILTER_CRITERIA["min_partants"])
            & (df["nb_partants"] <= FILTER_CRITERIA["max_partants"])
        ]

    # Filtrer par historique du cheval si colonne existe
    if "nb_courses_cheval" in df.columns:
        df = df[df["nb_courses_cheval"] >= FILTER_CRITERIA["min_horse_history"]]

    final_count = len(df)
    filtered = initial_count - final_count

    if filtered > 0:
        logger.info(f"üîç Filtr√© {filtered:,} lignes ({filtered/initial_count*100:.1f}%)")

    return df


def prepare_features(df: pd.DataFrame, feature_cols: list = None):
    """Pr√©pare X et y pour l'entra√Ænement"""

    # Colonnes √† exclure
    target_cols = ["position_arrivee", "victoire", "place"]
    id_cols = [
        "id_performance",
        "id_course",
        "nom_norm",
        "date",
        "hippodrome",
        "course_id",
        "race_id",
        "cheval_id",
    ]

    exclude = target_cols + id_cols

    if feature_cols is None:
        feature_cols = [col for col in df.columns if col not in exclude]

    # S√©lectionner seulement les colonnes qui existent
    available_cols = [col for col in feature_cols if col in df.columns]

    X = df[available_cols].copy()

    # G√©rer les colonnes cat√©gorielles
    cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
    if cat_cols:
        from sklearn.preprocessing import LabelEncoder

        for col in cat_cols:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))

    # Remplacer NaN par 0
    X = X.fillna(0)

    # Target
    if "victoire" in df.columns:
        y = df["victoire"].values
    else:
        y = None

    return X, y, available_cols


# ============================================================================
# ENTRA√éNEMENT
# ============================================================================


def train_conservative_model(X_train, y_train, X_val, y_val, params: dict = None):
    """Entra√Æne le mod√®le XGBoost conservateur"""

    logger.info("üöÄ ENTRA√éNEMENT MOD√àLE CONSERVATEUR")
    logger.info("=" * 60)

    if params is None:
        params = CONSERVATIVE_PARAMS.copy()

    # Afficher les hyperparam√®tres cl√©s
    logger.info("üìä Hyperparam√®tres conservateurs:")
    logger.info(f"   ‚Ä¢ max_depth: {params['max_depth']} (r√©duit)")
    logger.info(f"   ‚Ä¢ learning_rate: {params['learning_rate']} (lent)")
    logger.info(f"   ‚Ä¢ min_child_weight: {params['min_child_weight']} (√©lev√©)")
    logger.info(f"   ‚Ä¢ reg_alpha: {params['reg_alpha']} (L1 fort)")
    logger.info(f"   ‚Ä¢ reg_lambda: {params['reg_lambda']} (L2 fort)")

    # Cr√©er DMatrix XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)

    # Early stopping agressif
    evals = [(dtrain, "train"), (dval, "val")]

    start_time = time.time()

    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=params.get("n_estimators", 2000),
        evals=evals,
        early_stopping_rounds=100,  # Plus patient
        verbose_eval=False,
    )

    train_time = time.time() - start_time

    logger.info(f"‚è±Ô∏è  Temps d'entra√Ænement: {train_time:.1f}s")
    logger.info(f"üå≥ Nombre d'arbres: {model.best_iteration}")

    return model, train_time


def evaluate_model(model, X, y, dataset_name: str = "Test"):
    """√âvalue le mod√®le et retourne les m√©triques"""

    dmatrix = xgb.DMatrix(X)
    y_prob = model.predict(dmatrix)
    y_pred = (y_prob > 0.5).astype(int)

    metrics = {
        "dataset": dataset_name,
        "auc": roc_auc_score(y, y_prob),
        "brier_score": brier_score_loss(y, y_prob),
        "log_loss": log_loss(y, y_prob),
        "precision": precision_score(y, y_pred, zero_division=0),
        "recall": recall_score(y, y_pred, zero_division=0),
        "f1": f1_score(y, y_pred, zero_division=0),
        "n_samples": len(y),
        "n_positives": int(sum(y)),
        "positive_rate": float(sum(y) / len(y)),
    }

    logger.info(f"üìä M√©triques {dataset_name}:")
    logger.info(f"   üéØ AUC: {metrics['auc']:.4f}")
    logger.info(f"   üìâ Brier Score: {metrics['brier_score']:.4f}")
    logger.info(f"   üìà Precision: {metrics['precision']:.4f}")
    logger.info(f"   üìà Recall: {metrics['recall']:.4f}")
    logger.info(f"   üìà F1: {metrics['f1']:.4f}")

    return metrics


# ============================================================================
# SAUVEGARDE
# ============================================================================


def save_model(
    model, scaler, feature_names: list, metrics: dict, output_dir: Path = Path("data/models/safe")
):
    """Sauvegarde le mod√®le et les artefacts"""

    logger.info(f"üíæ SAUVEGARDE DES ARTEFACTS: {output_dir}")

    # Cr√©er le r√©pertoire
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. Mod√®le XGBoost
    model_path = output_dir / "xgboost_model.pkl"
    joblib.dump(model, model_path)
    logger.info(f"   ‚úÖ Mod√®le: {model_path}")

    # 2. Scaler
    if scaler is not None:
        scaler_path = output_dir / "feature_scaler.pkl"
        joblib.dump(scaler, scaler_path)
        logger.info(f"   ‚úÖ Scaler: {scaler_path}")

    # 3. Feature names
    features_path = output_dir / "feature_names.json"
    with open(features_path, "w") as f:
        json.dump(feature_names, f, indent=2)
    logger.info(f"   ‚úÖ Features: {features_path}")

    # 4. Metadata
    metadata = {
        "model_type": "xgboost_conservative",
        "version": "safe_v1.0",
        "created_at": datetime.now().isoformat(),
        "hyperparameters": {k: v for k, v in CONSERVATIVE_PARAMS.items() if not k.startswith("n_")},
        "filter_criteria": FILTER_CRITERIA,
        "metrics": metrics,
        "n_features": len(feature_names),
        "description": "Mod√®le conservateur pour strat√©gie de betting d√©fensive",
    }

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2, default=str)
    logger.info(f"   ‚úÖ Metadata: {metadata_path}")

    # 5. Imputer (copier depuis champion si existe)
    champion_imputer = Path("data/models/champion/feature_imputer.pkl")
    if champion_imputer.exists():
        import shutil

        shutil.copy(champion_imputer, output_dir / "feature_imputer.pkl")
        logger.info("   ‚úÖ Imputer: copi√© depuis champion")

    return output_dir


# ============================================================================
# MAIN
# ============================================================================


def main(dry_run: bool = False):
    """Pipeline principal d'entra√Ænement conservateur"""

    logger.info("üõ°Ô∏è ENTRA√éNEMENT MOD√àLE CONSERVATEUR 'SAFE'")
    logger.info("=" * 70)
    logger.info(f"üïê D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if dry_run:
        logger.info("‚ö†Ô∏è  MODE DRY-RUN: Pas de sauvegarde")

    try:
        # 1. Charger les donn√©es
        train_df, val_df, test_df = load_data()

        # 2. Appliquer filtres conservateurs
        logger.info("\nüîç APPLICATION DES FILTRES CONSERVATEURS")
        train_df = apply_conservative_filters(train_df)
        val_df = apply_conservative_filters(val_df)
        test_df = apply_conservative_filters(test_df)

        # 3. Pr√©parer les features
        logger.info("\nüìä PR√âPARATION DES FEATURES")
        X_train, y_train, feature_cols = prepare_features(train_df)
        X_val, y_val, _ = prepare_features(val_df, feature_cols)
        X_test, y_test, _ = prepare_features(test_df, feature_cols)

        logger.info(f"   ‚Ä¢ Features: {len(feature_cols)}")
        logger.info(f"   ‚Ä¢ Train: {len(X_train):,} samples")
        logger.info(f"   ‚Ä¢ Val: {len(X_val):,} samples")
        logger.info(f"   ‚Ä¢ Test: {len(X_test):,} samples")

        # 4. Normalisation
        logger.info("\nüîÑ NORMALISATION")
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_cols)
        X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=feature_cols)
        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_cols)

        # 5. Entra√Ænement
        model, train_time = train_conservative_model(X_train_scaled, y_train, X_val_scaled, y_val)

        # 6. √âvaluation
        logger.info("\nüìà √âVALUATION")
        metrics_train = evaluate_model(model, X_train_scaled, y_train, "Train")
        metrics_val = evaluate_model(model, X_val_scaled, y_val, "Validation")
        metrics_test = evaluate_model(model, X_test_scaled, y_test, "Test")

        all_metrics = {
            "train": metrics_train,
            "validation": metrics_val,
            "test": metrics_test,
            "train_time_seconds": train_time,
            "n_features": len(feature_cols),
            "best_iteration": model.best_iteration,
        }

        # 7. Sauvegarde
        if not dry_run:
            output_dir = save_model(
                model=model, scaler=scaler, feature_names=feature_cols, metrics=all_metrics
            )

            # Cr√©er aussi le r√©pertoire de calibration
            calib_dir = Path("calibration/safe")
            calib_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"üìÅ R√©pertoire calibration cr√©√©: {calib_dir}")

        # 8. R√©sum√©
        logger.info("\n" + "=" * 70)
        logger.info("üìã R√âSUM√â - MOD√àLE CONSERVATEUR SAFE")
        logger.info("=" * 70)
        logger.info(f"üéØ AUC Test: {metrics_test['auc']:.4f}")
        logger.info(f"üìâ Brier Score Test: {metrics_test['brier_score']:.4f}")
        logger.info(f"üìà Precision Test: {metrics_test['precision']:.4f}")
        logger.info(f"üìà F1 Score Test: {metrics_test['f1']:.4f}")
        logger.info(f"üå≥ Nombre d'arbres: {model.best_iteration}")
        logger.info(f"‚è±Ô∏è  Temps total: {train_time:.1f}s")

        if not dry_run:
            logger.info(f"\n‚úÖ Mod√®le sauvegard√©: {output_dir}")
            logger.info("üîú Prochaine √©tape: Calibrer avec calibration_pipeline.py")

        return model, all_metrics

    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Entra√Æne un mod√®le XGBoost conservateur")
    parser.add_argument("--dry-run", action="store_true", help="Mode test sans sauvegarde")
    args = parser.parse_args()

    main(dry_run=args.dry_run)
