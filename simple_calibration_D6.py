#!/usr/bin/env python3
"""
SIMPLE CALIBRATION D6 - BAS√âE SUR LE RAPPORT
============================================

Utilise les param√®tres du rapport de calibration pour appliquer
la calibration sans d√©pendre des objets pickle.
"""

import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime
from scipy.special import softmax
from sklearn.isotonic import IsotonicRegression

# Configuration
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


def load_calibration_params():
    """Charge les param√®tres de calibration depuis le rapport"""

    logger.info("üìä CHARGEMENT PARAM√àTRES DE CALIBRATION")

    # Charger le rapport le plus r√©cent
    report_path = "calibration/calibration_report_20251208_163949.json"

    with open(report_path, "r") as f:
        report = json.load(f)

    logger.info(f"‚úÖ Rapport charg√©: {report_path}")

    # Extraire les param√®tres
    params = {}

    # Temp√©rature
    if "stats" in report["metrics"] and "temperature" in report["metrics"]["stats"]:
        params["temperature"] = report["metrics"]["stats"]["temperature"]
        logger.info(f"üå°Ô∏è  Temp√©rature: {params['temperature']:.4f}")
    else:
        params["temperature"] = 1.0
        logger.warning("‚ö†Ô∏è  Temp√©rature non trouv√©e, utilisation de 1.0")

    # Gamma de correction
    if "debias" in report["metrics"] and "gamma" in report["metrics"]["debias"]:
        params["gamma"] = report["metrics"]["debias"]["gamma"]["global"]
        logger.info(f"üîß Gamma: {params['gamma']:.4f}")
    else:
        params["gamma"] = 1.0
        logger.warning("‚ö†Ô∏è  Gamma non trouv√©, utilisation de 1.0")

    # Alpha de blend
    if "debias" in report["metrics"] and "alpha_stats" in report["metrics"]["debias"]:
        params["alpha"] = report["metrics"]["debias"]["alpha_stats"]["mean"]
        logger.info(f"‚öñÔ∏è  Alpha: {params['alpha']:.4f}")
    else:
        params["alpha"] = 1.0  # Mod√®le uniquement
        logger.warning("‚ö†Ô∏è  Alpha non trouv√©, utilisation mod√®le uniquement")

    # Type de calibrateur
    if "stats" in report["metrics"] and "calibrator_type" in report["metrics"]["stats"]:
        params["calibrator_type"] = report["metrics"]["stats"]["calibrator_type"]
        logger.info(f"üìä Calibrateur: {params['calibrator_type']}")
    else:
        params["calibrator_type"] = "isotonic"
        logger.warning("‚ö†Ô∏è  Calibrateur non sp√©cifi√©, utilisation isotonic")

    return params


def simple_isotonic_calibration(probabilities, true_labels, test_probabilities):
    """Calibration isotonique simple r√©entra√Æn√©e"""

    logger.info("üéØ Calibration isotonique simple...")

    # Cr√©er et entra√Æner le calibrateur
    calibrator = IsotonicRegression(out_of_bounds="clip")

    # Entra√Ænement sur un √©chantillon
    sample_size = min(50000, len(probabilities))  # Limiter pour la m√©moire
    indices = np.random.choice(len(probabilities), sample_size, replace=False)

    calibrator.fit(probabilities[indices], true_labels[indices])

    # Application sur toutes les donn√©es de test
    calibrated = calibrator.predict(test_probabilities)

    logger.info("‚úÖ Calibration isotonique appliqu√©e")

    return calibrated


def apply_simple_calibration(df, params):
    """Applique une calibration simple bas√©e sur les param√®tres"""

    logger.info("üîÑ APPLICATION CALIBRATION SIMPLE")

    result_df = df.copy()

    # 1. NORMALISATION SOFTMAX PAR COURSE
    logger.info("üìê √âtape 1: Normalisation softmax par course")

    temperature = params["temperature"]

    def normalize_race_softmax(group):
        logits = group["logits_model"].values
        scaled_logits = logits / temperature
        probas = softmax(scaled_logits)
        return probas

    normalized_probs = []
    for race_id, group in df.groupby("race_id"):
        race_probs = normalize_race_softmax(group)
        normalized_probs.extend(race_probs)

    result_df["p_model_norm"] = normalized_probs

    logger.info("‚úÖ Normalisation softmax termin√©e")

    # 2. CALIBRATION APPROXIMATIVE
    logger.info("üéØ √âtape 2: Calibration approximative")

    # Pour la calibration, on va utiliser une approche simple
    # bas√©e sur les donn√©es d'entra√Ænement disponibles

    # S√©parer train/val pour la calibration
    train_data = result_df[result_df["split"] == "train"].copy()

    if len(train_data) > 10000:  # Assez de donn√©es pour calibrer
        # Calibration isotonique simple
        calibrated_probs = simple_isotonic_calibration(
            train_data["p_model_norm"].values,
            train_data["label_win"].values,
            result_df["p_model_norm"].values,
        )

        result_df["p_calibrated"] = calibrated_probs
        logger.info("‚úÖ Calibration isotonique appliqu√©e")
    else:
        # Pas assez de donn√©es, garder les probabilit√©s normalis√©es
        result_df["p_calibrated"] = result_df["p_model_norm"]
        logger.warning("‚ö†Ô∏è  Pas assez de donn√©es train, calibration ignor√©e")

    # 3. CORRECTION GAMMA DU MARCH√â
    logger.info("üîÄ √âtape 3: Correction gamma et blend")

    gamma = params["gamma"]
    alpha = params["alpha"]

    # Probabilit√©s march√© brutes
    p_market_raw = 1.0 / result_df["odds_market_preoff"]

    # Normalisation par course
    p_market_norm = []
    for race_id, group in result_df.groupby("race_id"):
        race_market_probs = 1.0 / group["odds_market_preoff"].values
        race_market_probs = race_market_probs / race_market_probs.sum()
        p_market_norm.extend(race_market_probs)

    result_df["p_market_norm"] = p_market_norm

    # Correction gamma
    p_market_corrected_raw = np.power(result_df["p_market_norm"], gamma)

    # Re-normalisation par course
    p_market_corrected = []
    for race_id, group in result_df.groupby("race_id"):
        group_indices = group.index
        group_corrected = p_market_corrected_raw[group_indices]
        group_normalized = group_corrected / group_corrected.sum()
        p_market_corrected.extend(group_normalized)

    result_df["p_market_corrected"] = p_market_corrected

    # Blend mod√®le/march√©
    result_df["p_blend"] = (
        alpha * result_df["p_calibrated"] + (1 - alpha) * result_df["p_market_corrected"]
    )

    # 4. RENORMALISATION FINALE
    logger.info("üîÑ √âtape 4: Renormalisation finale")

    p_final = []
    for race_id, group in result_df.groupby("race_id"):
        group_indices = group.index
        group_probs = result_df.loc[group_indices, "p_blend"].values
        group_normalized = group_probs / group_probs.sum()
        p_final.extend(group_normalized)

    result_df["p_final"] = p_final

    logger.info("‚úÖ Calibration compl√®te termin√©e")

    return result_df


def main():
    """Pipeline principal"""

    logger.info("üöÄ SIMPLE CALIBRATION D6")
    logger.info("=" * 70)

    try:
        # 1. Chargement des param√®tres
        params = load_calibration_params()

        # 2. Chargement des donn√©es
        logger.info("üìÇ Chargement des donn√©es adapt√©es...")
        df = pd.read_csv("data/backtest_predictions_adapted.csv")
        logger.info(f"‚úÖ Donn√©es charg√©es: {len(df):,} lignes")

        # 3. Application de la calibration
        result_df = apply_simple_calibration(df, params)

        # 4. Pr√©paration du fichier final
        logger.info("üìã Pr√©paration du fichier final...")

        output_df = pd.DataFrame()
        output_df["race_key"] = result_df["race_id"]
        output_df["id_cheval"] = result_df["id_cheval"]
        output_df["date_course"] = result_df["date_course"]
        output_df["p_model_win"] = result_df["p_model_win"]
        output_df["p_model_norm"] = result_df["p_model_norm"]
        output_df["p_calibrated"] = result_df["p_calibrated"]
        output_df["p_final"] = result_df["p_final"]
        output_df["is_win"] = result_df["label_win"]
        output_df["place"] = result_df["place"]
        output_df["position_arrivee"] = result_df["position_arrivee"]
        output_df["cote_sp"] = result_df["cote_sp"]
        output_df["split"] = result_df["split"]

        # 5. Sauvegarde
        output_path = "data/backtest_predictions_calibrated.csv"
        logger.info("üíæ Sauvegarde du fichier final...")

        output_df.to_csv(output_path, index=False)

        # 6. Statistiques finales
        logger.info("\nüìä STATISTIQUES FINALES:")
        logger.info("=" * 50)
        logger.info(f"üìÇ Fichier: {output_path}")
        logger.info(f"üìä Lignes: {len(output_df):,}")

        # V√©rification normalisation
        race_sums = output_df.groupby("race_key")["p_final"].sum()
        perfect_sums = (np.abs(race_sums - 1.0) < 1e-3).sum()
        logger.info(
            f"üîç Courses normalis√©es: {perfect_sums:,}/{len(race_sums):,} ({perfect_sums/len(race_sums)*100:.1f}%)"
        )

        # Distribution des probabilit√©s
        for col in ["p_model_win", "p_final"]:
            stats = output_df[col].describe()
            logger.info(
                f"üìà {col}: min={stats['min']:.4f}, m√©diane={stats['50%']:.4f}, max={stats['max']:.4f}"
            )

        # Corr√©lation
        corr = output_df["p_model_win"].corr(output_df["p_final"])
        logger.info(f"üîó Corr√©lation original/calibr√©: {corr:.4f}")

        logger.info("\nüéâ CALIBRATION D6 TERMIN√âE!")

    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
